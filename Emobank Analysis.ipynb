{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo = pd.read_csv('emobank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>split</th>\n",
       "      <th>V</th>\n",
       "      <th>A</th>\n",
       "      <th>D</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>110CYL068_1036_1079</td>\n",
       "      <td>train</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.20</td>\n",
       "      <td>Remember what she said in my last letter? \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>110CYL068_1079_1110</td>\n",
       "      <td>test</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2.80</td>\n",
       "      <td>If I wasn't working here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>110CYL068_1127_1130</td>\n",
       "      <td>train</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>..\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>110CYL068_1137_1188</td>\n",
       "      <td>train</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.22</td>\n",
       "      <td>Goodwill helps people get off of public assist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>110CYL068_1189_1328</td>\n",
       "      <td>train</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.46</td>\n",
       "      <td>Sherry learned through our Future Works class ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  split     V     A     D  \\\n",
       "0  110CYL068_1036_1079  train  3.00  3.00  3.20   \n",
       "1  110CYL068_1079_1110   test  2.80  3.10  2.80   \n",
       "2  110CYL068_1127_1130  train  3.00  3.00  3.00   \n",
       "3  110CYL068_1137_1188  train  3.44  3.00  3.22   \n",
       "4  110CYL068_1189_1328  train  3.55  3.27  3.46   \n",
       "\n",
       "                                                text  \n",
       "0        Remember what she said in my last letter? \"  \n",
       "1                          If I wasn't working here.  \n",
       "2                                                ..\"  \n",
       "3  Goodwill helps people get off of public assist...  \n",
       "4  Sherry learned through our Future Works class ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(x):\n",
    "  if x == 'dev' or x == 'test':\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo['split'] = emo['split'].apply(assign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    8062\n",
       "True     2000\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo['split'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself',\n",
    "            'yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself',\n",
    "            'they','them','their','theirs','themselves','what','which','who','whom','this','that',\n",
    "            'these','those','am','is','are','was','were','be','been','being','have','has','had',\n",
    "            'having','do','does','did','doing','a','an','the','and','but','if','or','because','as',\n",
    "            'until','while','of','at','by','for','with','about','against','between','into','through',\n",
    "            'during','before','after','above','below','to','from','up','down','in','out','on','off',\n",
    "            'over','under','again','further','then','once','here','there','when','where','why','how',\n",
    "            'all','any','both','each','few','more','most','other','some','such','no','nor','not',\n",
    "            'only','own','same','so','than','too','very','s','t','can','will','just','don','should',\n",
    "            'now','uses','use','using','used','one','also']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    reviews_tokens = []\n",
    "    for review in data:\n",
    "        review = review.lower() #Convert to lower-case words\n",
    "        raw_word_tokens = re.findall(r'(?:\\w+)', review,flags = re.UNICODE) #remove pontuaction\n",
    "        word_tokens = [w for w in raw_word_tokens if not w in stop_words] # do not add stop words\n",
    "        reviews_tokens.append(word_tokens)\n",
    "    return reviews_tokens #return all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo['text'] = preprocess(list(emo['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>split</th>\n",
       "      <th>V</th>\n",
       "      <th>A</th>\n",
       "      <th>D</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>110CYL068_1036_1079</td>\n",
       "      <td>False</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.20</td>\n",
       "      <td>[remember, said, last, letter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>110CYL068_1079_1110</td>\n",
       "      <td>True</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2.80</td>\n",
       "      <td>[wasn, working]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>110CYL068_1127_1130</td>\n",
       "      <td>False</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>110CYL068_1137_1188</td>\n",
       "      <td>False</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.22</td>\n",
       "      <td>[goodwill, helps, people, get, public, assista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>110CYL068_1189_1328</td>\n",
       "      <td>False</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.46</td>\n",
       "      <td>[sherry, learned, future, works, class, could,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  split     V     A     D  \\\n",
       "0  110CYL068_1036_1079  False  3.00  3.00  3.20   \n",
       "1  110CYL068_1079_1110   True  2.80  3.10  2.80   \n",
       "2  110CYL068_1127_1130  False  3.00  3.00  3.00   \n",
       "3  110CYL068_1137_1188  False  3.44  3.00  3.22   \n",
       "4  110CYL068_1189_1328  False  3.55  3.27  3.46   \n",
       "\n",
       "                                                text  \n",
       "0                     [remember, said, last, letter]  \n",
       "1                                    [wasn, working]  \n",
       "2                                                 []  \n",
       "3  [goodwill, helps, people, get, public, assista...  \n",
       "4  [sherry, learned, future, works, class, could,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZheMin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X = list(emo['text'])\n",
    "# let X be a list of tokenized texts (i.e. list of lists of tokens)\n",
    "model = gensim.models.Word2Vec(X, size=100)\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = 100\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = 100\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etree_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([\n",
    "    (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(X,word2vec):\n",
    "    dim = 100\n",
    "    return np.array([\n",
    "        np.mean([word2vec[w] for w in words if w in word2vec]\n",
    "                or [np.zeros(dim)], axis=0)\n",
    "        for words in X\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_transform(X,word2vec):\n",
    "    dim = 100\n",
    "    return np.array([\n",
    "        np.sum([word2vec[w] for w in words if w in word2vec]\n",
    "                or [np.zeros(dim)], axis=0)\n",
    "        for words in X\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: take mean of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_test=emo.loc[emo['split']==True]\n",
    "emo_train=emo.loc[emo['split']==False]\n",
    "x_train = transform(emo_train['text'],w2v)\n",
    "x_test = transform(emo_test['text'],w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(40, 20), learning_rate='adaptive',\n",
       "             learning_rate_init=0.001, max_iter=2000, momentum=0.9,\n",
       "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "             random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_V = MLPRegressor(solver='sgd', alpha=1e-5, learning_rate = 'adaptive',\n",
    "                     hidden_layer_sizes=(40, 20), random_state=1, max_iter = 2000)\n",
    "clf_V.fit(x_train,emo_train.V)\n",
    "\n",
    "clf_A = MLPRegressor(solver='sgd', alpha=1e-5, learning_rate = 'adaptive',\n",
    "                     hidden_layer_sizes=(40, 20), random_state=1, max_iter = 2000)\n",
    "clf_A.fit(x_train,emo_train.A)\n",
    "\n",
    "clf_D = MLPRegressor(solver='sgd', alpha=1e-5, learning_rate = 'adaptive',\n",
    "                     hidden_layer_sizes=(40, 20), random_state=1, max_iter = 2000)\n",
    "clf_D.fit(x_train,emo_train.D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for V: 0.34334346005690697\n",
      "RMSE for A: 0.24913387803853304\n",
      "RMSE for D: 0.20710904640617162\n"
     ]
    }
   ],
   "source": [
    "V_pred = clf_V.predict(x_test)\n",
    "print(\"RMSE for V:\",np.sqrt(mean_squared_error(emo_test.V, V_pred)))\n",
    "A_pred = clf_A.predict(x_test)\n",
    "print(\"RMSE for A:\",np.sqrt(mean_squared_error(emo_test.A, A_pred)))\n",
    "D_pred = clf_D.predict(x_test)\n",
    "print(\"RMSE for D:\",np.sqrt(mean_squared_error(emo_test.D, D_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: take sum of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_test=emo.loc[emo['split']==True]\n",
    "emo_train=emo.loc[emo['split']==False]\n",
    "x_train = sum_transform(emo_train['text'],w2v)\n",
    "x_test = sum_transform(emo_test['text'],w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(40, 20), learning_rate='adaptive',\n",
       "             learning_rate_init=0.001, max_iter=2000, momentum=0.9,\n",
       "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "             random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_V = MLPRegressor(solver='sgd', alpha=1e-5, learning_rate = 'adaptive',\n",
    "                     hidden_layer_sizes=(40, 20), random_state=1, max_iter = 2000)\n",
    "clf_V.fit(x_train,emo_train.V)\n",
    "\n",
    "clf_A = MLPRegressor(solver='sgd', alpha=1e-5, learning_rate = 'adaptive',\n",
    "                     hidden_layer_sizes=(40, 20), random_state=1, max_iter = 2000)\n",
    "clf_A.fit(x_train,emo_train.A)\n",
    "\n",
    "clf_D = MLPRegressor(solver='sgd', alpha=1e-5, learning_rate = 'adaptive',\n",
    "                     hidden_layer_sizes=(40, 20), random_state=1, max_iter = 2000)\n",
    "clf_D.fit(x_train,emo_train.D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for V: 0.3458218124052251\n",
      "RMSE for A: 0.2502452449486823\n",
      "RMSE for D: 0.21046637538693508\n"
     ]
    }
   ],
   "source": [
    "V_pred = clf_V.predict(x_test)\n",
    "print(\"RMSE for V:\",np.sqrt(mean_squared_error(emo_test.V, V_pred)))\n",
    "A_pred = clf_A.predict(x_test)\n",
    "print(\"RMSE for A:\",np.sqrt(mean_squared_error(emo_test.A, A_pred)))\n",
    "D_pred = clf_D.predict(x_test)\n",
    "print(\"RMSE for D:\",np.sqrt(mean_squared_error(emo_test.D, D_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing individual phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"A fundraiser dedicated to you and you having fun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.97369838])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_V.predict(np.mean(transform(preprocess([sample_text])[0],w2v),axis=0).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.06400376])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_A.predict(np.mean(transform(preprocess([sample_text])[0],w2v),axis=0).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.07429187])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_D.predict(np.mean(transform(preprocess([sample_text])[0],w2v),axis=0).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
